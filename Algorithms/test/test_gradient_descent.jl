using Statistics
using LinearAlgebra
using Test

include("../src/gradient_descent.jl")

@testset "Gradient descent" begin

  mutable struct Decay i end

  Base.:*(Î´Î·::Decay, X) = X / âˆš(Î´Î·.i += 1)

  # src. AI a modern approach
  loss_squared(X, y, ğ°, Ï†) = (ğ° â‹… Ï†(X) - y)^2  # â‹… == \cdot from Linear Algebra
  âˆ‡loss_squared(X, y, ğ°, Ï†) = -2 * (y - ğ° â‹… Ï†(X)) * X
  Î¼_loss(ğ°, ğ’Ÿtrain, Ï†, loss_fn) = mean(loss_fn(X, y, ğ°, Ï†) for (X, y) âˆˆ ğ’Ÿtrain)

  idâ‚“ = x -> x

  """
  Single-dimensional training input data.
  """
  function test_gradient_descent()
    ğ’Ÿtrain = [(3, 4), (-1, 3), (-1, 0)]
    ğ°_opt = âˆ‡_descent(ğ’Ÿtrain, idâ‚“, âˆ‡loss_squared)
    y_opt = Î¼_loss(ğ°_opt, ğ’Ÿtrain, idâ‚“, loss_squared)
    (ğ°_opt, y_opt)
  end

  """
         Multi-dimensional training data input.
  """
  function test_gradient_descent_multi()
    ğ’Ÿtrain = [([3., 0.7], 4.), ([-1., 0.3], 3.), ([-1., -3.], 0.)]
    ğ°_opt = âˆ‡_descent(ğ’Ÿtrain, idâ‚“, âˆ‡loss_squared)
    y_opt = Î¼_loss(ğ°_opt, ğ’Ÿtrain, idâ‚“, loss_squared)
    (ğ°_opt, y_opt)
  end

  ğ°, y = test_gradient_descent()
  @test ğ° â‰ˆ [0.8181818181818182]
  @test y â‰ˆ 5.878787878787879


  ğ°, y = test_gradient_descent_multi()
  @test ğ° â‰ˆ [0.8314306533883896, -0.03036191401505953]
  @test y â‰ˆ 5.876487733786738
end
