using Statistics
using LinearAlgebra
using Random
using Test

include("../src/gradient_descent.jl")

@testset "Gradient descent" begin

  mutable struct Decay i end

  Base.:*(Î´Î·::Decay, X) = X / âˆš(Î´Î·.i += 1)

  # src. AI a modern approach
  loss_squared(X, y, ğ°, Ï†) = (ğ° â‹… Ï†(X) - y)^2  # â‹… == \cdot from Linear Algebra
  âˆ‡loss_squared(X, y, ğ°, Ï†) = -2 * (y - ğ° â‹… Ï†(X)) * X
  Î¼_loss(ğ°, ğ’Ÿtrain, Ï†, loss_fn) = mean(loss_fn(X, y, ğ°, Ï†) for (X, y) âˆˆ ğ’Ÿtrain)

  idâ‚“ = x -> x

  ğ’Ÿtrainâ‚€ = [([3., 0.7], 4.), ([-1., 0.3], 3.), ([-1., -3.], 0.)]

  """
  Single-dimensional training input data.
  """
  function test_âˆ‡_descent_basic()
    ğ’Ÿtrain = [(3, 4), (-1, 3), (-1, 0)]
    ğ°_opt = âˆ‡_descent(ğ’Ÿtrain, idâ‚“, âˆ‡loss_squared)
    y_opt = Î¼_loss(ğ°_opt, ğ’Ÿtrain, idâ‚“, loss_squared)
    (ğ°_opt, y_opt)
  end

  """
  Multi-dimensional training data input.
  """
  function test_âˆ‡_descent_multi_basic()

    ğ°_opt = âˆ‡_descent(ğ’Ÿtrainâ‚€, idâ‚“, âˆ‡loss_squared)
    y_opt = Î¼_loss(ğ°_opt, ğ’Ÿtrainâ‚€, idâ‚“, loss_squared)
    (ğ°_opt, y_opt)
  end

  """
  (again) Multi-dimensional training data input.
  """
  function test_stochastic_âˆ‡_descent_basic()
    ğ°_opt = stochastic_âˆ‡_descent(ğ’Ÿtrainâ‚€, idâ‚“, âˆ‡loss_squared; Î·=0.01)
    y_opt = Î¼_loss(ğ°_opt, ğ’Ÿtrainâ‚€, idâ‚“, loss_squared)
    (ğ°_opt, y_opt)
  end

  function test_stochastic_âˆ‡_descent(;Î·=0.01, N=1_000, seed=70)
    Random.seed!(seed)
    X = randn(100, 5)                                    # Create random dataset with 100 rows and 5 columns
    y = (idâ‚“.(X) * [5, 3, 1, 2, 4]) .+ randn(100) .* 0.1 # Create corresponding target value by adding random noise

    ğ’Ÿtrain = [(X[ix, :], y[ix]) for ix âˆˆ 1:size(X, 1)]
    ğ°_opt = stochastic_âˆ‡_descent(ğ’Ÿtrain, idâ‚“, âˆ‡loss_squared; Î·=Î·, N=N)
    y_opt = Î¼_loss(ğ°_opt, ğ’Ÿtrain, idâ‚“, loss_squared)
    (ğ°_opt, y_opt)
  end

  function test_minibatch_stochastic_âˆ‡_descent(;Î·=0.01, N=1_000, seed=70)
    Random.seed!(seed)
    X = randn(100, 5)                                     # Create random dataset with 100 rows and 5 columns
    y = (idâ‚“.(X) * [5, 3, 1, 2, 4]) .+ randn(100) .* 0.1  # Create corresponding target value by adding random noise in the dataset

    ğ’Ÿtrain = [(X[ix, :], y[ix]) for ix âˆˆ 1:size(X, 1)]
    ğ°_opt = minibatch_stochastic_âˆ‡_descent(ğ’Ÿtrain, idâ‚“, âˆ‡loss_squared; Î·=Î·, N=N)
    y_opt = Î¼_loss(ğ°_opt, ğ’Ÿtrain, idâ‚“, loss_squared)
    (ğ°_opt, y_opt)
  end


  ğ°, y = test_âˆ‡_descent_basic()
  @test ğ° â‰ˆ [0.8181818181818182]
  @test y â‰ˆ 5.878787878787879


  ğ°, y = test_âˆ‡_descent_multi_basic()
  @test ğ° â‰ˆ [0.8314306533883896, -0.03036191401505953]
  @test y â‰ˆ 5.876487733786738


  ğ°, y = test_stochastic_âˆ‡_descent_basic()
  @test ğ° â‰ˆ [0.8286254042639639, -0.07376670863696766]
  @test y â‰ˆ 5.8829223883616395

  Ïµ = 1.e-6
  isapprox(a, b; rtol = Ïµ, kwargs...) = Base.isapprox(a, b; rtol=rtol, kwargs...)


  ğ°, y = test_stochastic_âˆ‡_descent(N=10_000)
  @test ğ° â‰ˆ [4.994195281768873, 2.9888364454480634, 0.9895028912070879, 1.998500451626844, 3.9940672737357628] # rtol = Ïµ
  @test y â‰ˆ 0.009403064501924843 # rtol = Ïµ


  ğ°, y = test_minibatch_stochastic_âˆ‡_descent(N=10_000)
  @test ğ° â‰ˆ [4.995754913210971, 2.9854258315124715, 0.9986456733903936, 1.987942491820608, 3.9987465936663265] rtol = Ïµ
  @test y â‰ˆ0.009306890923146974 rtol = Ïµ

end
