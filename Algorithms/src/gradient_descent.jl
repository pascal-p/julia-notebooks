using Statistics
using LinearAlgebra



"""
    âˆ‡_descent(ğ’Ÿtrain, Ï†, âˆ‡loss; Î·=0.1, T=100)

Perform stochastic gradient descent on a provided dataset `ğ’Ÿtrain`.
- `ğ’Ÿtrain`: The training dataset (X, y), X the matrix of input, y (a vector) representing the target (ground truth)
- `Ï†`:       The initial point or starting parameter values
- `âˆ‡loss`:   The gradient of the loss function to minimize
- `Î·`:       The learning rate (default 0.1)
- `N`:       The max number of iterations (default 100)

The function will return the updated values for Ï†.

Note:
  "ğ’Ÿ" can be typed by \\scrD<tab>
  "Ï†" can be typed by \\varphi<tab>
"""
function âˆ‡_descent(ğ’Ÿtrain, Ï†, âˆ‡loss; Î·=0.1, N=100) # where T <: Number
  ğ° = (Ï†(ğ’Ÿtrain[1][1]) |> length |> rand) # starting from any point in the parameter space
  for _ âˆˆ 1:N
    ğ° .-= Î· * mean(âˆ‡loss(X, y, ğ°, Ï†) for (X, y) âˆˆ ğ’Ÿtrain)
  end
  ğ°
end

# function âˆ‡_descent(ğ’Ÿtrain::AbstractVector{Tuple{T, T}}, Ï†, âˆ‡loss; Î·=0.1, N=100) where T <: Number

# function âˆ‡_descent(ğ’Ÿtrain::AbstractVector{Tuple{Vector{T}, T}}, Ï†, âˆ‡loss; Î·=0.1, N=100) where T <: Number
#   ğ° = Ï†(ğ’Ÿtrain[1][1]) |> length |> rand
#   for _ âˆˆ 1:N
#     ğ° .-= Î· * mean(âˆ‡loss(X, y, ğ°, Ï†) for (X, y) âˆˆ ğ’Ÿtrain)
#   end
#   ğ°
# end

