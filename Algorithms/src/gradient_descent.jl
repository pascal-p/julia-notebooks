using Statistics
using LinearAlgebra



"""
    âˆ‡_descent(ğ’Ÿtrain, Ï†, âˆ‡loss; Î·=0.1, T=100)

Perform gradient descent (GD) on a provided dataset `ğ’Ÿtrain`.
- `ğ’Ÿtrain`: The training dataset (X, y), X the matrix of input, y (a vector) representing the target (ground truth)
- `Ï†`:       The initial point or starting parameter values
- `âˆ‡loss`:   The gradient of the loss function to minimize
- `Î·`:       The learning rate (default 0.1)
- `N`:       The max number of iterations (default 100)

The function will return the updated values for Ï†.

Note:
  "ğ’Ÿ" can be typed by \\scrD<tab>
  "Ï†" can be typed by \\varphi<tab>
"""
function âˆ‡_descent(ğ’Ÿtrain, Ï†, âˆ‡loss; Î·=0.1, N=100)
  ğ° = (Ï†(ğ’Ÿtrain[1][1]) |> length |> rand) # starting from any point in the parameter space
  for _ âˆˆ 1:N
    ğ° .-= Î· * mean(âˆ‡loss(X, y, ğ°, Ï†) for (X, y) âˆˆ ğ’Ÿtrain)
  end
  ğ°
end

"""
    stochastic_âˆ‡_descent(ğ’Ÿtrain, Ï†, âˆ‡loss; Î·=0.1, T=100)

Perform stochastic gradient descent (SGD) on a provided dataset `ğ’Ÿtrain`.
- `ğ’Ÿtrain`: The training dataset (X, y), X the matrix of input, y (a vector) representing the target (ground truth)
- `Ï†`:       The initial point or starting parameter values
- `âˆ‡loss`:   The gradient of the loss function to minimize
- `Î·`:       The learning rate (default 0.1)
- `N`:       The max number of iterations (default 100)

The function will return the updated values for Ï†.
"""
function stochastic_âˆ‡_descent(ğ’Ÿtrain, Ï†, âˆ‡loss; Î·=0.1, N=500)
  ğ° = (Ï†(ğ’Ÿtrain[1][1]) |> length |> rand) # starting from any point in the parameter space
  for _ âˆˆ 1:N
    # make an update for each datapoint (the key difference between SGD and GD)
    for (X, y) âˆˆ ğ’Ÿtrain
      ğ° .-= Î· * âˆ‡loss(X, y, ğ°, Ï†)
    end
  end
  ğ°
end
