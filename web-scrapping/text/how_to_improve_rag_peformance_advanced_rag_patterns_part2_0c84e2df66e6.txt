- Title: How to improve RAG peformance — Advanced RAG Patterns — Part2
- Author and date: Ozgur Guler
·
Follow
12 min read
·
Oct 18, 2023
- Link: https://cloudatlas.me/how-to-improve-rag-peformance-advanced-rag-patterns-part2-0c84e2df66e6
Main:
How to improve RAG peformance — Advanced RAG Patterns — Part2
Ozgur Guler
·
Follow
12 min read
·
Oct 18, 2023
--
6
Listen
Share
In the realm of experimental Large Language Models (LLMs), creating a captivating LLM Minimum Viable Product (MVP) is relatively straightforward, but achieving production-level performance can be a formidable task, especially when it comes to building a high-performing Retrieval-Augmented Generation (RAG) pipeline for in-context learning. This post, part of the “Advanced RAG Patterns” series, delves into strategies and provides in-depth insights to enhance the performance of your RAG application. Subsequent posts will focus on implementing these strategies.
Understanding the Challenges
Before we explore the strategies, let’s dissect the critical challenges that contribute to suboptimal RAG system performance, classifying them into three distinct categories:
(For common issues with retrieval, augmentation and generation that will result in suboptimal RAG performance please refer to my earlier post
Why do RAG pipelines fail? Advanced RAG Patterns — Part1
).)
1. Retrieval Problems:
Semantic Ambiguity:
Ambiguities in query interpretation.
Vector Similarity Issues:
Challenges with vector similarity measures like cosine similarity.
Granularity Mismatches:
Mismatches in the level of granularity between query and retrieved content.
Vector Space Density:
Irregularities in vector space distribution.
Sparse Retrieval Challenges:
Difficulty in retrieving relevant content due to sparse data.
2. Augmentation Problems:
Mismatched Context:
Content integration issues.
Redundancy:
Repeated information.
Improper Ranking:
Incorrect ranking of retrieved content.
Stylistic Inconsistencies:
Inconsistencies in writing style.
Over-reliance on Retrieved Content:
Heavy reliance on retrieved content, sometimes at the expense of original generation.
3. Generation Problems:
Logical Inconsistencies:
Contradictions or illogical statements.
Verbosity:
Excessive verbosity in generated content.
Over-generalization:
Providing overly generalized information.
Lack of Depth:
Superficial content.
Error Propagation:
Errors originating from retrieved data.
Stylistic Issues:
Stylistic inconsistencies.
Failure to Reconcile Contradictions:
Inability to resolve conflicting information.In this post we will focus more on the methods to fix RAG pipelines…
Strategies for Enhanced Performance
1. Data
For a high-performing RAG system, the data needs to be clean, consistent, and context-rich. Text should be standardized to remove special characters and irrelevant information, thereby enhancing retriever efficiency. Entities and terms should be disambiguated for consistency, while duplicate or redundant information should be eliminated to streamline retriever focus. Factuality is key; each piece of data should be validated for accuracy when possible. Implementing domain-specific annotations can add another layer of context, and incorporating a user feedback loop for continuous updates ensures the system adapts to real-world interactions. Time-sensitive topics require a mechanism to refresh outdated documents. Overall, the emphasis should be on clarity, context, and correctness to make the system efficient and reliable. Here is a list of best practices…
Text Cleaning: Standardize text format, remove special characters, and irrelevant information. This improves retriever efficiency and avoids garbage-in-garbage-out.
Entity Resolution: Disambiguate entities and terms for consistent referencing. For example, standardize “ML,” “Machine Learning,” and “machine learning” to a common term.
Data Deduplication: Remove duplicate documents or redundant information to enhance retriever focus and efficiency.
Document Segmentation: Break down long documents into manageable chunks, or conversely, combine small snippets into coherent documents to optimize retriever performance.
Domain-Specific Annotations: Annotate documents with domain-specific tags or metadata. For instance, given your cloud tech focus, you could tag cloud-related technologies like “AWS,” “Azure,” etc.
Data Augmentation: Use synonyms, paraphrasing, or even translation to/from other languages to increase the diversity of your corpus.
Hierarchy & Relationships: Identify parent-child or sibling relationships between documents to improve contextual understanding.
User Feedback Loop: Continuously update your database with new Q&A pairs based on real-world interactions, marking them for factual correctness.
Time-Sensitive Data: For topics that are frequently updated, implement a mechanism to invalidate or update outdated documents.
2. Embeddings
OpenAI’s embeddings are fixed-size and non-fine-tunable. With fixed OpenAI embeddings, the emphasis would indeed be on optimising other parts of your RAG pipeline — like the retrieval mechanism or the quality of your data corpus — to ensure that you’re making the most out of the embeddings you have.
If your embeddings model is fine-tunable you may take advantage of fine-tuning the embedding model, dynamic embeddings or
fine-tuning embeddings (with fine-tunable/trainable embeddings)
Fine-tuning embeddings within RAG has a direct bearing on its efficacy. By adapting embeddings to domain specifics, the retrieval step becomes sharper, ensuring the content fetched is highly relevant to the query. This fine-tuned retrieval acts as a more accurate foundation for the subsequent generation step. Especially in specialized domains, or when dealing with evolving or rare terms, these tailored embeddings are pivotal. In essence, for RAG, fine-tuning embeddings is akin to tuning the ears before letting the voice speak, ensuring what’s heard (retrieved) optimally influences what’s said (generated).
At the moment you cannot fine-tune ada-embedding-02. bge embedding models like bge-large-en; developed by the Beijing Academy of Artificial Intelligence (BAAI) are fine-tunable and high performant embedding models. You can use LLaMa Index to fine-tune bge embedding models [
*
]. To create the training data to fine-tune bge models, you first create questions for your document chunks using an LLM like gpt-35-turbo. The question and the document chunk (the answer) become fine-tuning pairs for your fine-tuning.
Dynamic embeddings (with fine-tunable/trainable embeddings)
Dynamic embeddings adjust according to the context in which a word appears, unlike static embeddings that represent each word with a single vector. For instance, in transformer models like BERT, the same word can have different embeddings depending on the surrounding words.
There is also empirical evidence that OpenAI’s embeddings model text-embedding-ada-002 model gives unexpectedly high cosine similarity results when length of the text is shot e.g. <5 tokens. Ideally we should ensure the embeddings text will have as much context around it as possible so that embedding gives “healthy” results.
OpenAI’s
embeddings-ada-02
model is based on the principles of large language models like GPT. It is more advanced than static embedding models and can capture some level of context. This means the embeddings it generates are influenced by the surrounding text to a certain degree. However, it’s important to note that while it captures context better than static models, it might not be as context-sensitive as the latest full-scale language models like GPT-4.
Refresh embeddings (with fine-tunable/trainable embeddings)
The embeddings should also be periodically refreshed to capture evolving semantics in your corpus. The goal is to make them efficient for both retrieval and matching, ensuring a speedy and accurate RAG implementation.
3. Retrieval
To enhance retrieval efficiency in your RAG system, adopt a holistic strategy. Start by refining your chunking process, exploring various sizes to strike the right balance. Embed metadata for improved filtering capabilities and context enrichment. Embrace query routing across multiple indexes, catering to diverse query types. Consider Langchain’s multi-vector retrieval method, which employs smaller chunks, summary embeddings, and hypothetical questions to bolster retrieval accuracy. Address vector similarity issues with re-ranking, and experiment with hybrid search and recursive retrieval techniques for performance gains. Strategies like HyDE and iterative approaches such as “Read Retrieve Read” offer promising results. Lastly, fine-tune your vector search algorithm, optimizing the trade-off between accuracy and latency. This comprehensive approach ensures your RAG system excels in retrieving relevant and contextually rich information.
Tune your chunking
Our aim is to collect as much relevant context and as little noise as possible. Chunk with small, medium & large size and use an evaluation framework like “LlamaIndex Response Evaluation” to decide on the optimal chunk size which uses GPT4 to evaluate faithfulness and relevancy to rate and compare seperate chunk sizes.
When building a RAG system, always remember that chunk_size is a pivotal parameter. Invest the time to meticulously evaluate and adjust your chunk size for unmatched results.
LLaMA index has an automated evaluation capability for different chunking methods…(Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex [
link
]).
embed references (metadata) to your chunks —
such as date & use for filtering. Adding chapter, sub-chapter references might be helpful metadata to improve retrieval too.
query routing over multiple indexes
— This works hands in hand with the previous approaches with metadata filtering and e.g. chunking. You may have different indexes and query them at the same time. If the query is a pointed query you may use your standard index or if it is a keyword search or filtering based on metadata such as a certain ‘date’ then you may use the relevant seperate index.
Langchain’s
multi-vector retrieval
is one such method. The methods to create multiple vectors per document include:
Smaller chunks: split a document into smaller chunks, and embed those along with the longer chunks.
Add “summary embeddings” — create a summary for each document, embed that along with (or instead of) the document.
Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document.
Re-ranking —
vector similiary search for embeddings might not interpret to semantic similarity . With rerenking your can address this discrepency.
Explore hybrid search —
By intelligently blending techniques such as keyword-based search, semantic search, and vector search, you can harness the advantages of each approach. This approach allows your RAG system to adapt to varying query types and information needs, ensuring that it consistently retrieves the most relevant and contextually rich information. Hybrid search can be a powerful addition to your retrieval strategy, enhancing the overall performance of your RAG pipeline.
Recursive retrieval & query engine —
Another powerful approach to optimize retrieval in your RAG system is to implement recursive retrieval and a sophisticated query engine. Recursive retrieval involves fetching smaller document chunks during initial retrieval to capture key semantic meaning. Later in the process, provide larger chunks with more contextual information to your Language Model (LM). This two-step retrieval method helps strike a balance between efficiency and context-rich responses.
Complementing this strategy is a robust query engine. A well-designed query engine is essential for interpreting user queries effectively, especially when they involve nuanced or complex language. It enables your RAG system to iteratively evaluate the question for missing information, formulating a more comprehensive response once all relevant details are available.
The combination of recursive retrieval and a smart query engine can significantly enhance the performance of your RAG system, ensuring that it retrieves not just relevant but contextually complete information for more accurate and informative answers.
HyDE
:
HyDE
is a strategy which takes a query, generates a hypothetical response, and then uses both for embedding look up. Researches have found this can dramatically improve performance.
“Read Retrieve Read” / ReAct ,
iteratively evaluate the question for missing information and formulate a response once all information is available.
Parent Document Retriever ,
fetch small chunks during retrieval to better capture semantic meaning, provide larger chunks with more context to your LLM
Vector Search —
Tune your vector search algorithm and parameters,
find the right balance between accuracy and latency.
When it comes to vector search in your RAG system, precision and speed are key. Start by fine-tuning the vector search algorithm and parameters, focusing on factors like the number of neighbors to search for and the distance metric used. The goal is to strike the right balance between accuracy and latency. Experiment with different configurations and benchmark their impact on retrieval efficiency.
Stay updated on the latest advancements in vector search algorithms and libraries, as new options frequently emerge. Additionally, consider implementing query batching to enhance search efficiency. By optimizing vector search, you ensure that your RAG system responds both accurately and swiftly to user queries, a critical factor for an efficient and responsive pipeline.
4. Synthesis
‘Synthesis,’ explores advanced techniques to refine your RAG system. We delve into query transformations, the art of decomposing complex queries into manageable sub-queries, a proven strategy for enhancing Large Language Models’ (LLMs) effectiveness. Additionally, we address the critical aspect of engineering the base prompt, where prompt templating and conditioning play a pivotal role in tailoring your RAG system’s behavior to specific use-cases and contexts. Together, these strategies elevate the precision and efficiency of your RAG pipeline.
Query transformations
— Split complex questions into multiple questions (llamaindex). Sub-queries: LLMs tend to work better when they break down complex queries. You can build this into your RAG system such that a query is decomposed into multiple questions.
Engineer your base prompt
Engineering the base prompt in a RAG system is crucial for guiding the model’s behavior. A two-fold approach: prompt templating and prompt conditioning.
Prompt Templating: Define a template that captures the essence of the query and context, keeping in mind the specific use-case. For instance, if you’re building a tech support bot, the template might look like: “Help the user resolve issue: {issue_description}. Consider these documents: {document_snippets}.”
Prompt Conditioning: You can also condition the model by adding a prefix that sets the context or instructs the model to answer in a certain way. Given your interest in machine learning, for example, you could prepend with “Using your understanding of machine learning and cloud technologies, answer the following:”
Here’s a simplified Python example for creating such a prompt, assuming you’ve got your query and retrieved documents:
# Your question and retrieved documents
question = "What is the best ML algorithm for classification?"
retrieved_docs = ["Doc1: SVM is widely used...", "Doc2: Random Forest is robust..."]
# Template
template = "Help answer the following question based on these documents: {question}. Consider these documents: {docs}"
# Construct the full prompt
full_prompt = template.format(question=question, docs=" ".join(retrieved_docs))
# Now, this `full_prompt` can be fed into the RAG generator
As additional improvements consider fine-tuning the base model and use function calling…
Fine-tuning & RAG
Fine-tuning just the generator in a RAG setup (e.g. fine-tuning the base gpt model the context and the prompt is being sent to) aims to improve the language generation component without touching the retrieval part. Doing this can have several benefits:
Answer Quality: Directly improves how well the generator formulates answers.
Contextual Understanding: Fine-tuning on domain-specific datasets can help the generator understand the context provided by the retriever more accurately.
Speed: It can make the generator more efficient, thus speeding up the entire RAG operation.
Function calling & RAG
The function calling feature can significantly enhance Retrieval-Augmented Generation (RAG) pipelines by introducing structured, actionable output during the generation step. This allows for real-time API integrations for up-to-date answers, optimized query execution to reduce errors, and modular retrieval methods for improved relevance. It can also facilitate a feedback loop for dynamic document fetching and offer structured JSON output for multi-step reasoning or data aggregation. Overall, it makes the RAG system more dynamic, accurate, and responsive.
Conclusion
In the world of RAG systems, optimizing performance is an ongoing journey. By carefully managing data, fine-tuning embeddings, enhancing retrieval strategies, and utilizing advanced synthesis techniques, you can push the boundaries of what your RAG application can achieve. Stay curious, innovative, and adaptive in this ever-evolving landscape.
Next we will cover implementations with AzureOpenAI on AzureML PromptFlow…
Follow me on medium
if you find the post helpful…
Subscribe to A
zureOpenAI Builders Newsletter
where we cover the lates on building with #AzureOpenAI on LinkedIn
here
.
References:
10 Ways to Improve the Performance of Retrieval Augmented Generation Systems [
link
]
LLaMa Index documentation — Building Performant RAG Applications for Production [
link
]
LLaMa Index documentation — Fine-tuning embeddings [
link
]
LLaMa Index blog — Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex [
link
]
As discussed in
arXiv:2212.10496
, HyDE remains competitive even when compared to fine-tuned models. However, it is important to test whether this approach is beneficial for your specific scenario.
https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector
Llm
Llmops
Generative Ai Solution
Gpt 4
OpenAI
--
--
6
Follow
Written by
Ozgur Guler
331 Followers
What I cannot create, I do not understand...Aspiring GenAI builder...
Follow
Help
Status
About
Careers
Blog
Privacy
Terms
Text to speech
Teams
```code
# Your question and retrieved documents
question = "What is the best ML algorithm for classification?"
retrieved_docs = ["Doc1: SVM is widely used...", "Doc2: Random Forest is robust..."]
# Template
template = "Help answer the following question based on these documents: {question}. Consider these documents: {docs}"
# Construct the full prompt
full_prompt = template.format(question=question, docs=" ".join(retrieved_docs))
# Now, this `full_prompt` can be fed into the RAG generator
```
 Links:
 - ("https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F0c84e2df66e6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------", "Open in app")
 - ("https://medium.com/?source=---two_column_layout_nav----------------------------------", "")
 - ("https://medium.com/search?source=---two_column_layout_nav----------------------------------", "")
 - ("https://medium.com/@343544/why-do-rag-pipelines-fail-advanced-rag-patterns-part1-841faad8b3c2", "Why do RAG pipelines fail? Advanced RAG Patterns — Part1")
 - ("http://Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "link")
 - ("https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector", "multi-vector retrieval")
 - ("http://boston.lti.cs.cmu.edu/luyug/HyDE/HyDE.pdf", "HyDE")
 - ("https://www.linkedin.com/build-relation/newsletter-follow?entityUrn=7057325620778680320", "zureOpenAI Builders Newsletter")
 - ("https://cdn-images-1.medium.com/max/800/1*JkMflzX04rmTDxv4Nia-lQ.png", "here")
 - ("https://gpt-index.readthedocs.io/en/latest/end_to_end_tutorials/dev_practices/production_rag.html?utm_source=bensbites&utm_medium=newsletter&utm_campaign=open-ai-fights-back-in-court", "link")
 - ("https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5", "link")
 - ("https://arxiv.org/abs/2212.10496", "arXiv:2212.10496")
 - ("https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector", "https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector")
 - ("https://medium.com/tag/llm?source=post_page-----0c84e2df66e6---------------llm-----------------", "Llm")
 - ("https://medium.com/tag/llmops?source=post_page-----0c84e2df66e6---------------llmops-----------------", "Llmops")
 - ("https://medium.com/tag/generative-ai-solution?source=post_page-----0c84e2df66e6---------------generative_ai_solution-----------------", "Generative Ai Solution")
 - ("https://medium.com/tag/gpt-4?source=post_page-----0c84e2df66e6---------------gpt_4-----------------", "Gpt 4")
 - ("https://medium.com/tag/openai?source=post_page-----0c84e2df66e6---------------openai-----------------", "OpenAI")
 - ("https://medium.statuspage.io/?source=post_page-----0c84e2df66e6--------------------------------", "Status")
 - ("https://blog.medium.com/?source=post_page-----0c84e2df66e6--------------------------------", "Blog")
 - ("https://speechify.com/medium?source=post_page-----0c84e2df66e6--------------------------------", "Text to speech")
 - ("https://medium.com/business?source=post_page-----0c84e2df66e6--------------------------------", "Teams")